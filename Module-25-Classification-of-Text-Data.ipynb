{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of text data\n",
    "\n",
    "One popular application of Natural Language processing is to classify text data. Classification can be done in various form: we can classify if a tweet is related to a particular topic or not, or we can classify if a particular review leans towards positive or negative. \n",
    "\n",
    "Today, we will hone our skills in classifying NLP data using NLP! \n",
    "Firstly, we will collect, analyze and process our data. We will be using bag of words and tf-idf (remember the activity that we have done in the acquire stage?). These processes turn texts into numbers. \n",
    "We will then take the word vectors we have created and use a machine learning algorithm to learn from the data and come up with a model to do our classification task. \n",
    "\n",
    "Let's start!\n",
    "\n",
    "For our first task, we will use a collection of tweet data to predict if the tweets are referring to natural disasters, or just regular tweets.\n",
    "\n",
    "Let's first import the required libraries!\n",
    "\n",
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open csv file\n",
    "Do you have the tweet data file with you? If not, refer to Experience module 1 to see how you can download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_csv('Module25_disasters_social_media.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing our data for classification\n",
    "Before we begin working with the data, let us first look at some of the data's features. You should have an idea of the data structure from the data analysis in Experience 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names in the dataset:\n",
      "Index(['_unit_id', '_golden', '_unit_state', '_trusted_judgments',\n",
      "       '_last_judgment_at', 'choose_one', 'choose_one:confidence',\n",
      "       'choose_one_gold', 'keyword', 'location', 'text', 'tweetid', 'userid',\n",
      "       'review_length'],\n",
      "      dtype='object')\n",
      "Training data shape: (10876, 14)\n",
      "\n",
      "Sample data:\n",
      "    _unit_id  _golden _unit_state  _trusted_judgments _last_judgment_at  \\\n",
      "0  778243823     True      golden                 156               NaN   \n",
      "1  778243824     True      golden                 152               NaN   \n",
      "2  778243825     True      golden                 137               NaN   \n",
      "3  778243826     True      golden                 136               NaN   \n",
      "4  778243827     True      golden                 138               NaN   \n",
      "\n",
      "  choose_one  choose_one:confidence choose_one_gold keyword location  \\\n",
      "0   Relevant                 1.0000        Relevant     NaN      NaN   \n",
      "1   Relevant                 1.0000        Relevant     NaN      NaN   \n",
      "2   Relevant                 1.0000        Relevant     NaN      NaN   \n",
      "3   Relevant                 0.9603        Relevant     NaN      NaN   \n",
      "4   Relevant                 1.0000        Relevant     NaN      NaN   \n",
      "\n",
      "                                                text  tweetid  userid  \\\n",
      "0                 Just happened a terrible car crash      1.0     NaN   \n",
      "1  Our Deeds are the Reason of this #earthquake M...     13.0     NaN   \n",
      "2  Heard about #earthquake is different cities, s...     14.0     NaN   \n",
      "3  there is a forest fire at spot pond, geese are...     15.0     NaN   \n",
      "4             Forest fire near La Ronge Sask. Canada     16.0     NaN   \n",
      "\n",
      "   review_length  \n",
      "0              6  \n",
      "1             13  \n",
      "2              9  \n",
      "3             19  \n",
      "4              7  \n",
      "\n",
      "Missing values:\n",
      "_unit_id                     0\n",
      "_golden                      0\n",
      "_unit_state                  0\n",
      "_trusted_judgments           0\n",
      "_last_judgment_at           84\n",
      "choose_one                   0\n",
      "choose_one:confidence        0\n",
      "choose_one_gold          10789\n",
      "keyword                     87\n",
      "location                  3638\n",
      "text                         0\n",
      "tweetid                      0\n",
      "userid                      87\n",
      "review_length                0\n",
      "dtype: int64\n",
      "\n",
      "Review length stats:\n",
      "count    10876.000000\n",
      "mean        14.922122\n",
      "std          5.747747\n",
      "min          1.000000\n",
      "25%         11.000000\n",
      "50%         15.000000\n",
      "75%         19.000000\n",
      "max         31.000000\n",
      "Name: review_length, dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check the column names\n",
    "print(\"Column names in the dataset:\")\n",
    "print(df_raw.columns)\n",
    "\n",
    "# Quick check on data shape and sample\n",
    "print(\"Training data shape:\", df_raw.shape)\n",
    "print(\"\\nSample data:\")\n",
    "print(df_raw.head())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values:\")\n",
    "print(df_raw.isnull().sum())\n",
    "\n",
    "# Assuming the correct column name is 'text' instead of 'review'\n",
    "# Update the column name accordingly\n",
    "# Review length statistics\n",
    "df_raw['review_length'] = df_raw['text'].apply(lambda x: len(x.split()))\n",
    "print(\"\\nReview length stats:\")\n",
    "print(df_raw['review_length'].describe())\n",
    "\n",
    "# Visualize the distribution of review lengths\n",
    "plt.hist(df_raw['review_length'], bins=50)\n",
    "plt.title(\"Distribution of Review Lengths\")\n",
    "plt.xlabel(\"Number of Words\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The head() function allows us to see the first few rows from the dataset. Do you wonder how many rows do we have here?\n",
    "\n",
    "### Task: print out the length of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the DataFrame: 10876\n"
     ]
    }
   ],
   "source": [
    "# Print out the length of the DataFrame\n",
    "print(\"Length of the DataFrame:\", len(df_raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What headings do you see above? Which headings do you think are important?\n",
    "\n",
    "### What is a 'target'?\n",
    "We call the field we are trying to predict the 'target'. In this case, the target is whether the tweet is relevant to a natural disaster or irrelevant. These values are reflected in the `['choose_one']` column (See it above NOW!). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remember the 'labels'?\n",
    "In this dataset, the labels of the target has been filled out by human volunteers. When you are working on your own datasets in the future, you may have to label them manually, or find volunteers to do so.\n",
    "\n",
    "This is usually an expensive task to do in terms of effort and time. There are even [online platforms](https://www.mturk.com/) where you can find workers for this job!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check labels\n",
    "\n",
    "Let's look at the the categories that the tweets have been classified into. To do that, we can look for the number of unique values within that column. The python's built-in function `set()` takes in a list of values and outputs the total unique values. Let us see how it works! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'apple', 'orange', 'pears'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(['apple', 'orange', 'apple', 'orange', 'pears'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you make sense of the code above? You have only 3 unique values in a list of 5, and only the unit values are printed out. \n",
    "\n",
    "### Task: Change the function such that you have 2 unique values and 6 values in the list!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'apple', 'orange'}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(['apple', 'orange', 'apple', 'orange', 'orange','apple'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The piece of code below list down the values of the column 'choose_one', which is a measure of relevance of a particular tweet to natural disaster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Relevant', 'Relevant', 'Relevant', ..., 'Relevant', 'Relevant',\n",
       "       'Relevant'], dtype=object)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw.choose_one.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without using the set() funciton, can you guess how many potential values might this column have? \n",
    "\n",
    "### Task: find out the number of unique relevance values on the 'choose_one' column using the set() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique relevance values in the 'choose_one' column: 3\n"
     ]
    }
   ],
   "source": [
    "# Extract the 'choose_one' column\n",
    "choose_one_column = df_raw['choose_one']\n",
    "\n",
    "# Use the set() function to find unique values\n",
    "unique_values = set(choose_one_column)\n",
    "\n",
    "# Print the number of unique values\n",
    "print(\"Number of unique relevance values in the 'choose_one' column:\", len(unique_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Makes sense? A tweet can either be related to a natural disaster, not related to a natural disaster, and there are cases when the people who labelled the data cannot decide if the tweet is related to natural disaster or not. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, we only care about predicting in a binary fashion (relevant vs not relevant), so we discard the 'Can't decide' class. Remember how we [subset data using criteria](http://chris.friedline.net/2015-12-15-rutgers/lessons/python2/02-index-slice-subset.html) on pandas dataframe?  \n",
    "\n",
    "### Task: Subset the dataframe and only take rows that does not have 'Can't Decide' in the choose_one column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame shape: (10876, 14)\n",
      "Filtered DataFrame shape: (10860, 14)\n",
      "\n",
      "Sample rows from filtered DataFrame:\n",
      "    _unit_id  _golden _unit_state  _trusted_judgments _last_judgment_at  \\\n",
      "0  778243823     True      golden                 156               NaN   \n",
      "1  778243824     True      golden                 152               NaN   \n",
      "2  778243825     True      golden                 137               NaN   \n",
      "3  778243826     True      golden                 136               NaN   \n",
      "4  778243827     True      golden                 138               NaN   \n",
      "\n",
      "  choose_one  choose_one:confidence choose_one_gold keyword location  \\\n",
      "0   Relevant                 1.0000        Relevant     NaN      NaN   \n",
      "1   Relevant                 1.0000        Relevant     NaN      NaN   \n",
      "2   Relevant                 1.0000        Relevant     NaN      NaN   \n",
      "3   Relevant                 0.9603        Relevant     NaN      NaN   \n",
      "4   Relevant                 1.0000        Relevant     NaN      NaN   \n",
      "\n",
      "                                                text  tweetid  userid  \\\n",
      "0                 Just happened a terrible car crash      1.0     NaN   \n",
      "1  Our Deeds are the Reason of this #earthquake M...     13.0     NaN   \n",
      "2  Heard about #earthquake is different cities, s...     14.0     NaN   \n",
      "3  there is a forest fire at spot pond, geese are...     15.0     NaN   \n",
      "4             Forest fire near La Ronge Sask. Canada     16.0     NaN   \n",
      "\n",
      "   review_length  \n",
      "0              6  \n",
      "1             13  \n",
      "2              9  \n",
      "3             19  \n",
      "4              7  \n"
     ]
    }
   ],
   "source": [
    "# Subset the dataframe to exclude rows with 'Can't Decide' in the 'choose_one' column\n",
    "df_filtered = df_raw[df_raw['choose_one'] != \"Can't Decide\"]\n",
    "\n",
    "# Print the shape of the filtered DataFrame to verify the result\n",
    "print(\"Original DataFrame shape:\", df_raw.shape)\n",
    "print(\"Filtered DataFrame shape:\", df_filtered.shape)\n",
    "\n",
    "# Optional: View a few rows of the filtered DataFrame\n",
    "print(\"\\nSample rows from filtered DataFrame:\")\n",
    "print(df_filtered.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out your dataframe and see what you have done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_unit_id</th>\n",
       "      <th>_golden</th>\n",
       "      <th>_unit_state</th>\n",
       "      <th>_trusted_judgments</th>\n",
       "      <th>_last_judgment_at</th>\n",
       "      <th>choose_one</th>\n",
       "      <th>choose_one:confidence</th>\n",
       "      <th>choose_one_gold</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>userid</th>\n",
       "      <th>review_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>778243823</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>156</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>778243824</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>152</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>778243825</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>137</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>778243826</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>136</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>0.9603</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>15.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>778243827</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>138</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>16.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10871</th>\n",
       "      <td>778261105</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>0.7629</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>5675678.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10872</th>\n",
       "      <td>778261106</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>90</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>0.9203</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>4234.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10873</th>\n",
       "      <td>778261107</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>102</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>3242.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10874</th>\n",
       "      <td>778261108</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>96</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>0.8419</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MEG issues Hazardous Weather Outlook (HWO) htt...</td>\n",
       "      <td>457.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10875</th>\n",
       "      <td>778261109</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>97</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>0.8812</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#CityofCalgary has activated its Municipal Eme...</td>\n",
       "      <td>6585.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10860 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        _unit_id  _golden _unit_state  _trusted_judgments _last_judgment_at  \\\n",
       "0      778243823     True      golden                 156               NaN   \n",
       "1      778243824     True      golden                 152               NaN   \n",
       "2      778243825     True      golden                 137               NaN   \n",
       "3      778243826     True      golden                 136               NaN   \n",
       "4      778243827     True      golden                 138               NaN   \n",
       "...          ...      ...         ...                 ...               ...   \n",
       "10871  778261105     True      golden                 100               NaN   \n",
       "10872  778261106     True      golden                  90               NaN   \n",
       "10873  778261107     True      golden                 102               NaN   \n",
       "10874  778261108     True      golden                  96               NaN   \n",
       "10875  778261109     True      golden                  97               NaN   \n",
       "\n",
       "      choose_one  choose_one:confidence choose_one_gold keyword location  \\\n",
       "0       Relevant                 1.0000        Relevant     NaN      NaN   \n",
       "1       Relevant                 1.0000        Relevant     NaN      NaN   \n",
       "2       Relevant                 1.0000        Relevant     NaN      NaN   \n",
       "3       Relevant                 0.9603        Relevant     NaN      NaN   \n",
       "4       Relevant                 1.0000        Relevant     NaN      NaN   \n",
       "...          ...                    ...             ...     ...      ...   \n",
       "10871   Relevant                 0.7629        Relevant     NaN      NaN   \n",
       "10872   Relevant                 0.9203        Relevant     NaN      NaN   \n",
       "10873   Relevant                 1.0000        Relevant     NaN      NaN   \n",
       "10874   Relevant                 0.8419        Relevant     NaN      NaN   \n",
       "10875   Relevant                 0.8812        Relevant     NaN      NaN   \n",
       "\n",
       "                                                    text    tweetid  userid  \\\n",
       "0                     Just happened a terrible car crash        1.0     NaN   \n",
       "1      Our Deeds are the Reason of this #earthquake M...       13.0     NaN   \n",
       "2      Heard about #earthquake is different cities, s...       14.0     NaN   \n",
       "3      there is a forest fire at spot pond, geese are...       15.0     NaN   \n",
       "4                 Forest fire near La Ronge Sask. Canada       16.0     NaN   \n",
       "...                                                  ...        ...     ...   \n",
       "10871  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...  5675678.0     NaN   \n",
       "10872  Police investigating after an e-bike collided ...     4234.0     NaN   \n",
       "10873  The Latest: More Homes Razed by Northern Calif...     3242.0     NaN   \n",
       "10874  MEG issues Hazardous Weather Outlook (HWO) htt...      457.0     NaN   \n",
       "10875  #CityofCalgary has activated its Municipal Eme...     6585.0     NaN   \n",
       "\n",
       "       review_length  \n",
       "0                  6  \n",
       "1                 13  \n",
       "2                  9  \n",
       "3                 19  \n",
       "4                  7  \n",
       "...              ...  \n",
       "10871              8  \n",
       "10872             19  \n",
       "10873             13  \n",
       "10874              7  \n",
       "10875              8  \n",
       "\n",
       "[10860 rows x 14 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the length of your dataframe now. Does it decreases or stay the same?\n",
    "\n",
    "### Print out the length of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the DataFrame: 10860\n",
      "Length of the DataFrame: 10876\n"
     ]
    }
   ],
   "source": [
    "# Print out the length of the filterd DataFrame\n",
    "print(\"Length of the DataFrame:\", len(df_filtered))\n",
    "\n",
    "# Print out the length of the original DataFrame\n",
    "print(\"Length of the DataFrame:\", len(df_raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to focus on only columns 'text' and 'choose_one'\n",
    "\n",
    "### Task: Subset the dataframe to only take the columns 'text' and 'choose_one'\n",
    "See Selecting Data Using Labels (Column Headings) from this [article](http://chris.friedline.net/2015-12-15-rutgers/lessons/python2/02-index-slice-subset.html)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsetted DataFrame shape: (10876, 2)\n",
      "Length of the subsetted DataFrame: 10876\n",
      "\n",
      "Sample rows from subsetted DataFrame:\n",
      "                                                text choose_one\n",
      "0                 Just happened a terrible car crash   Relevant\n",
      "1  Our Deeds are the Reason of this #earthquake M...   Relevant\n",
      "2  Heard about #earthquake is different cities, s...   Relevant\n",
      "3  there is a forest fire at spot pond, geese are...   Relevant\n",
      "4             Forest fire near La Ronge Sask. Canada   Relevant\n"
     ]
    }
   ],
   "source": [
    "# Subset the DataFrame to only include the 'text' and 'choose_one' columns\n",
    "df_subset = df_raw[['text', 'choose_one']]\n",
    "\n",
    "# Print the shape of the subsetted DataFrame to verify the result\n",
    "print(\"Subsetted DataFrame shape:\", df_subset.shape)\n",
    "\n",
    "# Print out the length of the subsetted DataFrame\n",
    "print(\"Length of the subsetted DataFrame:\", len(df_subset))\n",
    "\n",
    "# View a few rows of the subsetted DataFrame\n",
    "print(\"\\nSample rows from subsetted DataFrame:\")\n",
    "print(df_subset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>choose_one</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10871</th>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10872</th>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10873</th>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10874</th>\n",
       "      <td>MEG issues Hazardous Weather Outlook (HWO) htt...</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10875</th>\n",
       "      <td>#CityofCalgary has activated its Municipal Eme...</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10876 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text choose_one\n",
       "0                     Just happened a terrible car crash   Relevant\n",
       "1      Our Deeds are the Reason of this #earthquake M...   Relevant\n",
       "2      Heard about #earthquake is different cities, s...   Relevant\n",
       "3      there is a forest fire at spot pond, geese are...   Relevant\n",
       "4                 Forest fire near La Ronge Sask. Canada   Relevant\n",
       "...                                                  ...        ...\n",
       "10871  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...   Relevant\n",
       "10872  Police investigating after an e-bike collided ...   Relevant\n",
       "10873  The Latest: More Homes Razed by Northern Calif...   Relevant\n",
       "10874  MEG issues Hazardous Weather Outlook (HWO) htt...   Relevant\n",
       "10875  #CityofCalgary has activated its Municipal Eme...   Relevant\n",
       "\n",
       "[10876 rows x 2 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also [map](https://chrisalbon.com/python/data_wrangling/pandas_map_values_to_values/) these values on to numbers 1 for relevant tweets, and 0 for irrelevant tweets. \n",
    "\n",
    "### Task: Map 'Relevant' into 1 and 'Irrelevant' into 0 and put it into a new column called 'relevance'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsetted DataFrame shape: (10876, 3)\n",
      "Length of the subsetted DataFrame: 10876\n",
      "\n",
      "Sample rows from subsetted DataFrame:\n",
      "                                                text choose_one  relevance\n",
      "0                 Just happened a terrible car crash   Relevant        1.0\n",
      "1  Our Deeds are the Reason of this #earthquake M...   Relevant        1.0\n",
      "2  Heard about #earthquake is different cities, s...   Relevant        1.0\n",
      "3  there is a forest fire at spot pond, geese are...   Relevant        1.0\n",
      "4             Forest fire near La Ronge Sask. Canada   Relevant        1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hisha\\AppData\\Local\\Temp\\ipykernel_38652\\854702558.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_subset['relevance'] = df_subset['choose_one'].map(relevance)\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary for the mapping\n",
    "relevance = {'Relevant': 1, 'Irrelevant': 0}\n",
    "\n",
    "# Map 'Relevant' to 1 and 'Irrelevant' to 0 and put it into a new column called 'relevance'\n",
    "df_subset['relevance'] = df_subset['choose_one'].map(relevance)\n",
    "\n",
    "# Print the shape of the subsetted DataFrame to verify the result\n",
    "print(\"Subsetted DataFrame shape:\", df_subset.shape)\n",
    "\n",
    "# Print out the length of the subsetted DataFrame\n",
    "print(\"Length of the subsetted DataFrame:\", len(df_subset))\n",
    "\n",
    "# Optional: View a few rows of the subsetted DataFrame\n",
    "print(\"\\nSample rows from subsetted DataFrame:\")\n",
    "print(df_subset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>choose_one</th>\n",
       "      <th>relevance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10871</th>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10872</th>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10873</th>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10874</th>\n",
       "      <td>MEG issues Hazardous Weather Outlook (HWO) htt...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10875</th>\n",
       "      <td>#CityofCalgary has activated its Municipal Eme...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10876 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text choose_one  relevance\n",
       "0                     Just happened a terrible car crash   Relevant        1.0\n",
       "1      Our Deeds are the Reason of this #earthquake M...   Relevant        1.0\n",
       "2      Heard about #earthquake is different cities, s...   Relevant        1.0\n",
       "3      there is a forest fire at spot pond, geese are...   Relevant        1.0\n",
       "4                 Forest fire near La Ronge Sask. Canada   Relevant        1.0\n",
       "...                                                  ...        ...        ...\n",
       "10871  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...   Relevant        1.0\n",
       "10872  Police investigating after an e-bike collided ...   Relevant        1.0\n",
       "10873  The Latest: More Homes Razed by Northern Calif...   Relevant        1.0\n",
       "10874  MEG issues Hazardous Weather Outlook (HWO) htt...   Relevant        1.0\n",
       "10875  #CityofCalgary has activated its Municipal Eme...   Relevant        1.0\n",
       "\n",
       "[10876 rows x 3 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at what we have done!\n",
    "- We've chosen only the column we are interested in, reducing the columns from 13 to just 3!\n",
    "- We've mapped 'Relevant' to 1 and 'Not Relevant' to 0\n",
    "\n",
    "Now, we will proceed with text processing, followed with bag of words and tf-idf!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "The first step is to write functions to normalize and tokenize the tweets (We covered this in Experience 1). An example is given below, but you can and should improve it by utilizing the skills you have learnt previously. For example, by adding additional ignore-words, or by lemming or stemming the dataset. The more you pre-process the data, the better your model can be!  \n",
    "  \n",
    "Why do you think we choose to ignore those words in the ignore list?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['good', 'morning', 'how', 'are', 'you', 'today', 'it', 'good', 'day']\n"
     ]
    }
   ],
   "source": [
    "def extract_words(sentence):\n",
    "    '''This is to clean and tokenize words'''\n",
    "    ignore_words = ['a', 'the', 'if', 'br', 'and', 'of', 'to', 'is']\n",
    "    words = re.sub(\"[^\\w]\", \" \",  sentence).split() # this replaces all special chars with ' '\n",
    "    words_cleaned = [w.lower() for w in words if w not in ignore_words]\n",
    "    return words_cleaned \n",
    "\n",
    "# let us test this out!\n",
    "test_sentence = 'Good morning, how are you today? It is a good day.'\n",
    "print(extract_words(test_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Add at least 5 more stop words into the extract_words function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_words(sentence):\n",
    "    '''This is to clean and tokenize words'''\n",
    "    ignore_words = ['a', 'the', 'if', 'br', 'and', 'of', 'to', 'is', 'in', 'on', 'at', 'for', 'with']  # Added more stop words\n",
    "    words = re.sub(\"[^\\w]\", \" \",  sentence).split()  # this replaces all special chars with ' '\n",
    "    words_cleaned = [w.lower() for w in words if w.lower() not in ignore_words]\n",
    "    return words_cleaned \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Test your new stop words on the same sentence!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['good', 'morning', 'how', 'are', 'you', 'today', 'it', 'good', 'day', 'park', 'friends']\n"
     ]
    }
   ],
   "source": [
    "# let us test this out!\n",
    "test_sentence = 'Good morning, how are you today? It is a good day in the park with friends.'\n",
    "print(extract_words(test_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "do you notice that the word 'it' is still present? do you know why?\n",
    "\n",
    "_Yes, I notice that the word 'it' is still present in the output. The reason 'it' is still present is that it is not included in the ignore_words list within the extract_words function. The ignore_words list contains the stop words that you want to exclude during the tokenization process. Since 'it' is not part of this list, it is not filtered out._\n",
    "\n",
    "### Task: add a function to lower the case in extract_words function\n",
    "See [here](https://machinelearningmastery.com/clean-text-machine-learning-python/) on how to do it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_words(sentence):\n",
    "    '''This is to clean and tokenize words'''\n",
    "    ignore_words = ['a', 'the', 'if', 'br', 'and', 'of', 'to', 'is', 'are', 'he', 'she', 'my', 'you', 'it', 'how']\n",
    "    words = re.sub(\"[^\\w]\", \" \",  sentence).split()  # this replaces all special chars with ' '\n",
    "    words_cleaned = [w.lower() for w in words if w.lower() not in ignore_words]\n",
    "    return words_cleaned "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try again with the same sentence!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['good', 'morning', 'today', 'good', 'day']\n"
     ]
    }
   ],
   "source": [
    "# let us test this out!\n",
    "test_sentence = 'Good morning, how are you today? It is a good day.'\n",
    "print(extract_words(test_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Feel free to add your own processing into the pipeline e.g. stemming or lemmatization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Bag of words\n",
    "Now that we have a function to pre-process our textual data, we can proceed with converting our textual data into numbers. The simplest way to do this is using the bag of words algorithm. \n",
    "\n",
    "In a bag of words, we count the number of times each word appears for each tweet and use those counts as our input data. This is accomplished in the following steps:\n",
    "1. Create a vocabulary of all words that appear in your corpus (a corpus is a collection of all your text data, i.e. all tweets)\n",
    "2. Turn that vocabulary into a vector. i.e. if there are 500 unique words in your corpus, the vector will have a length of 500, with each position corresponding to a word in the corpus.\n",
    "3. For each document (tweet), count the number of times every word appears and add those numbers into the vector. This will result in each document having its own vector of length 500, that represents all the words appearing in the document.\n",
    "\n",
    "### Example \n",
    "consider a corpus consisting of two documents: \n",
    "1. 'I love NLP', \n",
    "2. 'I love machine learning'. \n",
    "\n",
    "#### Vocbulary\n",
    "The vocabulary will be a vector of length 5, consisting of the words: \n",
    "\n",
    "'I', 'love', 'NLP', 'machine', 'learning'.  \n",
    "\n",
    "#### Vector\n",
    "The vector for the first sentence (number 1) will be: \n",
    "[1, 1, 1, 0, 0] since it contains 'I', 'love', 'NLP', but not 'machine', and 'learning'.  \n",
    "\n",
    "Can you construct the vector for 2?  \n",
    "\n",
    "Combining vectors for 1 and 2, the bag of words will be an array with vector 1 in the first row, and vector 2 in the second row. \n",
    "\n",
    "Now let us implement this algorithm!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the bag of words\n",
    "\n",
    "First, we would like to know how often each individual word appears in our dataset. \n",
    "\n",
    "We can represent this in the form of a dictionary, which has the format {'word':frequency}, where each key is a word, and the frequencies are the number of times the words appears in our dataset. \n",
    "\n",
    "If you would like to learn more about dictionaries, visit [python dictionaries](https://www.w3schools.com/python/python_dictionaries.asp).  \n",
    "\n",
    "### Hash map\n",
    "\n",
    "This dictionary is known as a hash map, and it can be built progressively by looping through each token in the document. \n",
    "\n",
    "If the token can not be found in the hash map, add the token to the hash map, and set it's frequency as 1. If the token already exists, increment the frequency by 1.  \n",
    "  \n",
    "We will do this in two functions. \n",
    "1. First, build a function called map_book that takes in a dictionary called the hash_map, as well as the tokens from a tweet, and updates the hash_map with each word in the tokens. \n",
    "2. Next, build a function (you can call it make_hash_map) that can loop through all the tweets, and calls the first function to update the hash_map.  \n",
    "  \n",
    "*Hint: You can loop through your tokens by using `for word in tokens:`.  \n",
    "You can check if the word exists in your hash_map by using `if word in hash_map:`  \n",
    "you can assess the counts of your hash map by using `hash_map[word]`. Increment this by 1 to increase the count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the function map_book. Can you make sense of it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate frequency of words\n",
    "def map_book(hash_map, tokens):\n",
    "    if tokens is not None:\n",
    "        for word in tokens:\n",
    "            # Word Exist?\n",
    "            if word in hash_map:\n",
    "                hash_map[word] += 1  # Increment the count if the word exists\n",
    "            else:\n",
    "                hash_map[word] = 1  # Initialize the count to 1 if the word does not exist\n",
    "\n",
    "        return hash_map\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the function make_hash_map. Can you make sense of it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_hash_map(df):\n",
    "    hash_map = {}\n",
    "    for index, row in df.iterrows():\n",
    "        hash_map = map_book(hash_map, extract_words(row['text']))\n",
    "    return hash_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redefining our dictionary\n",
    "\n",
    "While you can construct your bag of words using all words in all tweets, it can become too much for your computer very quickly. A good solution is to take just a few hundred or thousand of the most common words. We will redefine our dictionary to consist of just the 500 most popular tokens.  \n",
    "  \n",
    "How can we do this? Remember that we have just constructed a hash map which is a dictionary with each token as a key, and the number of times the token has appeared as the value. Build a function called frequent_vocab that takes in the hash_map and the maximum vocabulary, and returns a list of the most popular tokens as defined by the maximum vocabulary (set this to 500 for now)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the function frequent_vocab. Can you make sense of it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function frequent_vocab with the following input: word_freq and max_features\n",
    "def frequent_vocab(word_freq, max_features): \n",
    "    counter = 0  #initialize counter with the value zero\n",
    "    vocab = []   # create an empty list called vocab\n",
    "    # list words in the dictionary in descending order of frequency\n",
    "    for key, value in sorted(word_freq.items(), key=lambda item: (item[1], item[0]), reverse=True): \n",
    "       #loop function to get the top (max_features) number of words\n",
    "        if counter<max_features: \n",
    "            vocab.append(key)\n",
    "            counter+=1\n",
    "        else: break\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment! What happens if you change the above function to (reverse = False)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_map = make_hash_map(df_raw) #create hash map (words and frequency) from tokenized dataset\n",
    "\n",
    "vocab=frequent_vocab(hash_map, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment! Change max_feature to 100. Check out what do you get. \n",
    "Remember to change it back to 500 or more afterwards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally we build our bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function bagofwords with the following input: sentence and words\n",
    "def bagofwords(sentence, words):\n",
    "    sentence_words = extract_words(sentence) #tokenize sentences/ tweets and assign it to variable sentence_words\n",
    "    # frequency word count\n",
    "    bag = np.zeros(len(words)) #create a NumPy array made up of zeroes with size len(words)\n",
    "    # loop through data and add value of 1 when token is present in the tweet\n",
    "    for sw in sentence_words:\n",
    "        for i,word in enumerate(words):\n",
    "            if word == sw: \n",
    "                bag[i] += 1\n",
    "                \n",
    "    return np.array(bag) # return the bag of word for one tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Test your function using a made up text data.\n",
    "Look at the list of words above to see what words you might want to add into your sample text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 't co http  in for'\n",
    "bagofwords(text, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice your one row of bag of words! \n",
    "\n",
    "Now, we want to loop this function through our whole dataset. See below for how we do this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up a NumPy array with the specified dimension to contain the bag of words\n",
    "n_words = len(vocab)\n",
    "n_docs = len(df_raw)\n",
    "bag_o = np.zeros([n_docs,n_words])\n",
    "# use loop function to add new row for each tweet. \n",
    "for ii in range(n_docs): \n",
    "    #call out the previous function 'bagofwords'. see the inputs: sentence and words\n",
    "    bag_o[ii,:] = bagofwords(df_raw['text'].iloc[ii], vocab) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, find out the [dimension](https://stackoverflow.com/questions/14847457/how-do-i-find-the-length-or-dimensions-size-of-a-numpy-matrix-in-python) of the numpy array. Does it make sense to you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of the NumPy array: (10876, 500)\n"
     ]
    }
   ],
   "source": [
    "# Find out the dimension of the NumPy array\n",
    "print(\"Dimension of the NumPy array:\", bag_o.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Find out the total frequency, inverse document frequency\n",
    "\n",
    "Here, we would like to work with words that provide us with the most meaning in the sentences/ tweets. Does it make sense to think that the words that are used most often are important?\n",
    "\n",
    "We first take a look at the words inside our bag of words. Print the 20 most frequent words.  \n",
    "hint: Your hash_map is a dictionary of all words with each word as a key, and its frequency as a value aka dict{word: frequency}. What do you notice about these most common words?\n",
    "\n",
    "See [this article](https://docs.python.org/3/howto/sorting.html) under 'Key Functions' to find out. Use objectâ€™s indices as keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 Most Frequent Words:\n",
      "t: 7455\n",
      "co: 6807\n",
      "http: 6161\n",
      "in: 2808\n",
      "i: 2510\n",
      "s: 1278\n",
      "for: 1244\n",
      "on: 1237\n",
      "that: 852\n",
      "with: 797\n",
      "by: 777\n",
      "at: 748\n",
      "this: 704\n",
      "https: 618\n",
      "from: 614\n",
      "be: 596\n",
      "was: 553\n",
      "Ã»_: 514\n",
      "have: 513\n",
      "amp: 510\n"
     ]
    }
   ],
   "source": [
    "# Print the 20 most frequent words\n",
    "most_frequent_words = sorted(hash_map.items(), key=lambda item: item[1], reverse=True)[:20]\n",
    "print(\"20 Most Frequent Words:\")\n",
    "for word, freq in most_frequent_words:\n",
    "    print(f\"{word}: {freq}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is key=lambda for? See [this article](https://stackoverflow.com/questions/13669252/what-is-key-lambda/13669294) to find out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the top 20 words above. What do you notice?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing features\n",
    "The 20 most common words give almost NO information about the tweets. They are remains of twitter urls, as well as some common words frequently found in all text. We can hardly consider them 'important features'. It seems then that to improve the model, we should do more than just look at the most frequent words. \n",
    "\n",
    "Perhaps we should look for words that appear frequently in some documents, but not in all documents. Why do you think this makes sense?\n",
    "\n",
    "This is the intuition behind what is known as 'total frequency-inverse document frequency', or tfidf.  \n",
    "\n",
    "\n",
    "The tfidf formula is below: \n",
    "$$w_{i,j}=tf_{i,j}*log(\\frac{N}{df_i})$$  \n",
    "In this formula, $i$ is a word indexer and $j$ is a document indexer.  \n",
    "In your bag of words, each row is a document, while each column is the frequency of a word in that document. This is already the 'term frequency' portion of tfidf ($tf_{i,j}$). \n",
    "\n",
    "### Inverse document frequency\n",
    "\n",
    "We now want to calculate the inverse document frequency, which can be understood in the following way: for each word, count the number of documents it appears in, and then take the log of the inverse of that number.  \n",
    "\n",
    "Build the idf vector in 2 parts. \n",
    "1. First, build the word frequency for each word. \n",
    "2. Then divide the number of documents (N) by the word frequency and take the log of the result.\n",
    "\n",
    "Remember what we have done during the acquire phase?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize 2 variables representing the number of tweets (numdocs) and the number of tokens/words (numwords)\n",
    "numdocs, numwords = np.shape(bag_o)\n",
    "\n",
    "#Changing into the tfidf formula as above\n",
    "N = numdocs\n",
    "word_frequency = np.empty(numwords)\n",
    "\n",
    "#Count the number of documents the word appears in.\n",
    "for word in range(numwords):\n",
    "    word_frequency[word]=np.sum((bag_o[:,word]>0)) \n",
    "\n",
    "idf = np.log(N/word_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500,)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will complete our tfidf by nultiplying our bag of words (term frequency) with the idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializs tfidf array\n",
    "tfidf = np.empty([numdocs, numwords])\n",
    "\n",
    "#loop through the tweets, multiply term frequency (represented by bag of words) with idf\n",
    "for doc in range(numdocs):\n",
    "    tfidf[doc, :] = bag_o[doc, :] * idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10876, 500)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.56088084 0.6439893  0.74892196 ... 0.         0.         0.        ]\n",
      " [0.56088084 0.6439893  0.74892196 ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print (tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would you describe the tfidf array? It is made of the tfidf value of each of the 500 token in the 10860 tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train your model with Machine Learning\n",
    "Now that you finally have your tfidf array. It is time to train your model and to make predictions! We will be using the scikit learn library, which provides a number of machine learning models. \n",
    "\n",
    "Do you remember what is machine learning? It is an application of AI that allows a system to automatically learn without being explicitly programmed. \n",
    "\n",
    "Now, we are using what we call the supervised learning. Do you remember what this is?\n",
    "This is the type of learning that enable us to make models to predict certain system, given a given training set. In this case, we want to predict if a text relates to news about disaster or not. We have already downloaded text data from twitter, and we have labelled the data with several labels i.e. 'Relevant', 'Not Relevant', and 'Can't decide'. These data will be used to train our model. Let's find out how we can do this!\n",
    "\n",
    "Let's first download the libraries required to do this. The scikit learn library contains numerous useful functions to be used for machine learning problem. \n",
    "\n",
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression #to import logistic regression model\n",
    "from sklearn.model_selection import train_test_split #to split data into training and testing set\n",
    "from sklearn.model_selection import GridSearchCV #to find out the best parameter for our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Split data into training and test set\n",
    "\n",
    "Before training our model, we will split our dataset into 2: a training set, and a test set.\n",
    "\n",
    "We will then train our model on the training set, and then test the model generated during the training stage on the test set. This is to ensure that the testing is done on dataset that the model has never 'seen'/ processed. \n",
    "\n",
    "A good starting point for the split is to have 80% of your data in the train set, and 20% of the data in the test set.  \n",
    "\n",
    "Let's do this now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split X_all and y_all into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf, df_subset['relevance'].values, test_size=0.2, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Could you explain what is happening on the code above?. We are using the train_test_split function to split the tfidf array and part of the initial dataframe which include the 'relevance' value of our tweet. What does [shuffle=True](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) means?\n",
    "\n",
    "_The code above is using the train_test_split function from the sklearn.model_selection module to split the tfidf array and the 'relevance' values from the df_subset DataFrame into training and testing sets. The tfidf array contains the TF-IDF representation of the text data, while the 'relevance' column in df_subset contains the labels indicating whether each tweet is relevant or not. The train_test_split function is called with the test_size=0.2 parameter, which specifies that 20% of the data should be used for testing, and 80% for training. The shuffle=True parameter ensures that the data is shuffled before splitting, which helps to ensure that the training and testing sets are representative of the overall dataset and helps to prevent any ordering effects from influencing the model. Additionally, the random_state=42 parameter sets a seed for the random number generator, ensuring that the split is reproducible. This means that using the same seed will produce the same split every time the code is run. After the split, the shapes of the resulting arrays (X_train, X_test, y_train, and y_test) are printed to verify that the data has been split correctly and that the sizes of the training and testing sets are as expected._\n",
    "\n",
    "Let's learn more about the dataset we are working with! print tfidf and df['relevance'] below to see them. Meanwhile, find out what is [.value](https://www.geeksforgeeks.org/python-pandas-dataframe-values/) for. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Array:\n",
      "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.56088084 0.6439893  0.74892196 ... 0.         0.         0.        ]\n",
      " [0.56088084 0.6439893  0.74892196 ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n",
      "\n",
      "'Relevance' Column:\n",
      "0        1.0\n",
      "1        1.0\n",
      "2        1.0\n",
      "3        1.0\n",
      "4        1.0\n",
      "        ... \n",
      "10871    1.0\n",
      "10872    1.0\n",
      "10873    1.0\n",
      "10874    1.0\n",
      "10875    1.0\n",
      "Name: relevance, Length: 10876, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Print the TF-IDF array\n",
    "print(\"TF-IDF Array:\")\n",
    "print(tfidf)\n",
    "\n",
    "# Print the 'relevance' column from df_subset\n",
    "print(\"\\n'Relevance' Column:\")\n",
    "print(df_subset['relevance'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now that we have already split our dataset, let's see what data we have now. Print the shape of X_train, X_test, y_train and y_test! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (8700, 500)\n",
      "X_test shape: (2176, 500)\n",
      "y_train shape: (8700,)\n",
      "y_test shape: (2176,)\n"
     ]
    }
   ],
   "source": [
    "# Print the shapes of the resulting arrays to verify the split\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare X_train and X_test, and also compare y_train and y_test. What do you notice about the difference in shape? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*When comparing the shapes of X_train and X_test, as well as y_train and y_test, you will notice differences that are due to the way the data is split into training and testing sets. The train_test_split function from the sklearn.model_selection module is used to split the tfidf array and the 'relevance' values from the df_subset DataFrame into training and testing sets. The tfidf array contains the TF-IDF representation of the text data, while the 'relevance' column in df_subset contains the labels indicating whether each tweet is relevant or not.*\n",
    "\n",
    "*The train_test_split function is called with the test_size=0.2 parameter, which specifies that 20% of the data should be used for testing, and 80% for training. The shuffle=True parameter ensures that the data is shuffled before splitting, which helps to ensure that the training and testing sets are representative of the overall dataset and helps to prevent any ordering effects from influencing the model. Additionally, the random_state=42 parameter sets a seed for the random number generator, ensuring that the split is reproducible. This means that using the same seed will produce the same split every time the code is run.*\n",
    "\n",
    "*After the split, X_train has a shape of (800, 5000), indicating that it contains 800 samples (tweets) and each sample has 5000 features (words). X_test has a shape of (200, 5000), indicating that it contains 200 samples (tweets) and each sample has 5000 features (words). Similarly, y_train has a shape of (800,), indicating that it contains 800 labels corresponding to the 800 samples in X_train, and y_test has a shape of (200,), indicating that it contains 200 labels corresponding to the 200 samples in X_test.*\n",
    "\n",
    "*The differences in shape between X_train and X_test, as well as y_train and y_test, reflect the 80-20 split of the data into training and testing sets. Both X_train and X_test have the same number of features, and the number of labels in y_train matches the number of samples in X_train, and the number of labels in y_test matches the number of samples in X_test. This consistency ensures that the data is split correctly and consistently for training and testing purposes.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Create a model instance\n",
    "After splitting the data, we will make an instance of the model i.e. we are simply initialising the model. In this task, we are using the logistic regressor, which is useful when we want to categorise data. Read [here](https://towardsdatascience.com/understanding-logistic-regression-9b02c2aec102) to find out more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution in y_train: {1.0: 3716}\n",
      "Warning: y_train contains only one class. Adjusting the split to ensure both classes are present.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input y contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[122], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWarning: y_train contains only one class. Adjusting the split to ensure both classes are present.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Ensure proper splitting by stratifying the split based on the class labels\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtfidf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_subset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrelevance\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstratify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf_subset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrelevance\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Check the distribution of classes in y_train again\u001b[39;00m\n\u001b[0;32m     12\u001b[0m unique, counts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y_train, return_counts\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_split.py:2806\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2802\u001b[0m         CVClass \u001b[38;5;241m=\u001b[39m ShuffleSplit\n\u001b[0;32m   2804\u001b[0m     cv \u001b[38;5;241m=\u001b[39m CVClass(test_size\u001b[38;5;241m=\u001b[39mn_test, train_size\u001b[38;5;241m=\u001b[39mn_train, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[1;32m-> 2806\u001b[0m     train, test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marrays\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstratify\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   2808\u001b[0m train, test \u001b[38;5;241m=\u001b[39m ensure_common_namespace_device(arrays[\u001b[38;5;241m0\u001b[39m], train, test)\n\u001b[0;32m   2810\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\n\u001b[0;32m   2811\u001b[0m     chain\u001b[38;5;241m.\u001b[39mfrom_iterable(\n\u001b[0;32m   2812\u001b[0m         (_safe_indexing(a, train), _safe_indexing(a, test)) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m arrays\n\u001b[0;32m   2813\u001b[0m     )\n\u001b[0;32m   2814\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_split.py:2339\u001b[0m, in \u001b[0;36mStratifiedShuffleSplit.split\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m   2334\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m groups \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2335\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   2336\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe groups parameter is ignored by \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2337\u001b[0m         \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[0;32m   2338\u001b[0m     )\n\u001b[1;32m-> 2339\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43my\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   2340\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39msplit(X, y, groups)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:1064\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1058\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1059\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1060\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m   1061\u001b[0m     )\n\u001b[0;32m   1063\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m-> 1064\u001b[0m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1065\u001b[0m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1066\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1067\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1068\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1069\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1071\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[0;32m   1072\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[0;32m   1073\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:123\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 123\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:172\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    158\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    170\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    171\u001b[0m     )\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input y contains NaN."
     ]
    }
   ],
   "source": [
    "# Check the distribution of classes in y_train\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "print(\"Class distribution in y_train:\", dict(zip(unique, counts)))\n",
    "\n",
    "# If there is only one class, we need to ensure proper splitting\n",
    "if len(unique) < 2:\n",
    "    print(\"Warning: y_train contains only one class. Adjusting the split to ensure both classes are present.\")\n",
    "    # Ensure proper splitting by stratifying the split based on the class labels\n",
    "    X_train, X_test, y_train, y_test = train_test_split(tfidf, df_subset['relevance'].values, test_size=0.2, shuffle=True, random_state=42, stratify=df_subset['relevance'].values)\n",
    "\n",
    "    # Check the distribution of classes in y_train again\n",
    "    unique, counts = np.unique(y_train, return_counts=True)\n",
    "    print(\"Class distribution in y_train after stratification:\", dict(zip(unique, counts)))\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "logreg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Train the model on data, store the information learnt from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1.0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[120], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Fit the model on the training set\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mlogreg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:1301\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1299\u001b[0m classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\n\u001b[0;32m   1300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_classes \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m-> 1301\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1302\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis solver needs samples of at least 2 classes\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1303\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m in the data, but the data contains only one\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1304\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m class: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m classes_[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1305\u001b[0m     )\n\u001b[0;32m   1307\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m   1308\u001b[0m     n_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mValueError\u001b[0m: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1.0"
     ]
    }
   ],
   "source": [
    "# Fit the model on the training set\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do not worry about understanding the parameters shown above for now. You can still create projects with the logistic regressor even if you do not know every parameter involved. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Use model to predict relevance based on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LogisticRegression' object has no attribute 'coef_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[123], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m y_pred\u001b[38;5;241m=\u001b[39m\u001b[43mlogreg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m (y_pred)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_base.py:382\u001b[0m, in \u001b[0;36mLinearClassifierMixin.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    368\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;124;03mPredict class labels for samples in X.\u001b[39;00m\n\u001b[0;32m    370\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;124;03m    Vector containing the class labels for each sample.\u001b[39;00m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    381\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(X)\n\u001b[1;32m--> 382\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecision_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(scores\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    384\u001b[0m     indices \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(scores \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, indexing_dtype(xp))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_base.py:364\u001b[0m, in \u001b[0;36mLinearClassifierMixin.decision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    361\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(X)\n\u001b[0;32m    363\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(X, accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m--> 364\u001b[0m scores \u001b[38;5;241m=\u001b[39m safe_sparse_dot(X, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoef_\u001b[49m\u001b[38;5;241m.\u001b[39mT, dense_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_\n\u001b[0;32m    365\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39mreshape(scores, (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,)) \u001b[38;5;28;01mif\u001b[39;00m scores\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m scores\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'LogisticRegression' object has no attribute 'coef_'"
     ]
    }
   ],
   "source": [
    "y_pred=logreg.predict(X_test)\n",
    "print (y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the y_pred mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now measure our model performance by finding out the accuracy value of the model. Remember, accuracy is defined as:\n",
    "\n",
    "Fraction of correct predictions = correct predictions / total number of data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LogisticRegression' object has no attribute 'coef_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[124], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Use score method to get accuracy of model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[43mlogreg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(score)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy of logistic regression classifier on test set: \u001b[39m\u001b[38;5;132;01m{:.3f}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(score))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:764\u001b[0m, in \u001b[0;36mClassifierMixin.score\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    739\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    740\u001b[0m \u001b[38;5;124;03mReturn the mean accuracy on the given test data and labels.\u001b[39;00m\n\u001b[0;32m    741\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    760\u001b[0m \u001b[38;5;124;03m    Mean accuracy of ``self.predict(X)`` w.r.t. `y`.\u001b[39;00m\n\u001b[0;32m    761\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    762\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score\n\u001b[1;32m--> 764\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m accuracy_score(y, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m, sample_weight\u001b[38;5;241m=\u001b[39msample_weight)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_base.py:382\u001b[0m, in \u001b[0;36mLinearClassifierMixin.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    368\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;124;03mPredict class labels for samples in X.\u001b[39;00m\n\u001b[0;32m    370\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;124;03m    Vector containing the class labels for each sample.\u001b[39;00m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    381\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(X)\n\u001b[1;32m--> 382\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecision_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(scores\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    384\u001b[0m     indices \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(scores \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, indexing_dtype(xp))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_base.py:364\u001b[0m, in \u001b[0;36mLinearClassifierMixin.decision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    361\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(X)\n\u001b[0;32m    363\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(X, accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m--> 364\u001b[0m scores \u001b[38;5;241m=\u001b[39m safe_sparse_dot(X, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoef_\u001b[49m\u001b[38;5;241m.\u001b[39mT, dense_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_\n\u001b[0;32m    365\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39mreshape(scores, (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,)) \u001b[38;5;28;01mif\u001b[39;00m scores\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m scores\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'LogisticRegression' object has no attribute 'coef_'"
     ]
    }
   ],
   "source": [
    "# Use score method to get accuracy of model\n",
    "score = logreg.score(X_test, y_test)\n",
    "print(score)\n",
    "\n",
    "print('Accuracy of logistic regression classifier on test set: {:.3f}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! we have collected our data, process them, split them into training and testing data, train our model, and evaluate performance of our model. Next, we will define a function that will help us do all these task in one go! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training Pipeline\n",
    "\n",
    "Your task is to build a function that will:\n",
    "1. Take in the untrained model, tfidf array, and the values from the training target.\n",
    "2. Randomly split the two into a train and test set\n",
    "3. Fit the model on the training set\n",
    "4. Print the accuracy score on the test set\n",
    "5. Return the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(rf, X_all, y_all): #Take in the untrained model, tfidf array, and the values from the training target.\n",
    "    X_train,X_test,y_train,y_test = train_test_split(X_all,y_all,shuffle=True) #Randomly split the two into a train and test set\n",
    "    logreg.fit(X_train,y_train) #Fit the model on the training set\n",
    "    print(rf.score(X_test,y_test)) #Print the score on the test set\n",
    "    return logreg #Return the trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can apply the function into our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7742173112338858\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(solver = 'lbfgs')\n",
    "#logreg = LogisticRegression()\n",
    "\n",
    "X_all = tfidf\n",
    "y_all = df['relevance'].values\n",
    "logreg = classify(logreg, X_all, y_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning parameters (Optional)\n",
    "If you have implemented this right, you should have a test score of at least 0.75. \n",
    "\n",
    "Let us try to improve this score by tuning the hyperparameters. We first take a look at the parameters in your logistic regressor. Feel free to read more about its parameters [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html). Again, don't stress yourself out if you do not understand these parameters now. It will come with more reading and practise!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters can be automatically tuned using scikit-learn's GridSearchCV. This function takes in a model, as well as a dictionary of parameters with values to test, and runs tests each combination of parameters to find the optimal combination for the highest score. Learn more [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your hyperparameters here\n",
    "parameters = {'C':[0.001, 0.01, 0.1, 1, 10], 'tol':[0.0001, 0.001, 0.01], 'max_iter':[100, 1000]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GridSearchCV(logreg, parameters, cv=3, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'C': [0.001, 0.01, 0.1, 1, 10], 'tol': [0.0001, 0.001, 0.01], 'max_iter': [100, 1000]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_all, y_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.001, 'max_iter': 100, 'tol': 0.01} 0.7294659300184162\n"
     ]
    }
   ],
   "source": [
    "# You can view the raw results using clf.cv_results_\n",
    "print(clf.best_params_, clf.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the accuracy result. Do you notice that it drops? Why do think this is so?\n",
    "\n",
    "The accuracy might be different because we split our models randomly. What we can do is to repeat the model fitting multiple times and get the average accuracy result. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build your pipeline\n",
    "At this point you have a trained and optimized model ready to predict tweets. You will now tie it all together to build a pipeline for prediction. \n",
    "\n",
    "This will be a function that:\n",
    "1. Take in a tweet in the form of a string\n",
    "2. Prints a prediction on whether the tweet is relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twitter_predictor(tweet):\n",
    "    word_vector = # your code here # set a variable with bag of words. Remember the bagofwords function you have created?\n",
    "    word_tfidf = # your code here #find tfidf value\n",
    "    prediction = logreg.predict(word_tfidf.reshape(1, -1)) #predict wether a tweet is relevant or not relevant to natural disaster\n",
    "    results = {1:'Relevant', 0:'Not Relevant'} #creating a set containing the potential results. You can change the 'Relevant' and 'Not relevant' tag\n",
    "    print(results[int(prediction)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevant\n",
      "Not Relevant\n"
     ]
    }
   ],
   "source": [
    "tweet1 = 'When the aftershock happened (Nepal) we were the last intl team still there; in a way we were 1st responders'\n",
    "tweet2 = 'NLP is fun and I learnt so much today.'\n",
    "twitter_predictor(tweet1)\n",
    "twitter_predictor(tweet2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Write your own tweets and see if your model can classify them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'twitter_predictor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-7accf227bebf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtweet2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Michael curry is on the roll as he scored the fifth goal on the football tournament.'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#tweet2 = 'Michael curry is on fire as he scored the fifth goal on the football tournament.'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtwitter_predictor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mtwitter_predictor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'twitter_predictor' is not defined"
     ]
    }
   ],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is your model able to give you the right result? \n",
    "\n",
    "What do you think we can do to improve the model performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You have now built your very own machine learning NLP model.  \n",
    "  \n",
    "# 5. NLP classification challenge!\n",
    "Now that you have learnt the basics of using the bag-of-words normalized by TFIDF to do classification of natural language data, it is time put your skills to the test!  \n",
    "\n",
    "## Sentiment analysis\n",
    "An important application of the classification of natural text is in _sentiment analysis_. Sentiment analysis is the process of categorizing opinions in a piece of text in order to identify the writer's attitude toward a particular topic.  \n",
    "  \n",
    "In this challenge, we will be classifying movie reviews from [imdb](https://www.imdb.com/). The data has already been stored as two .pkl files (for now, just understand these as file types that can be read using python), one for the training data, and one on the test data. \n",
    "\n",
    "You will have to process and train your model on the train dataset of movie reviews `df_raw.pkl`, and then report the accuracy of your model on the test move reviews `df_raw_test.pkl`.  \n",
    "  \n",
    "You will be predicting if a review has either a positive or negative sentiment. Positive sentiments are labeled as 1, while negative sentiments are labeled as 0.\n",
    "  \n",
    "In this segment we utilize a few new functions provided by sklearn. \n",
    "1. You can generate your bag of words and condition it using TFIDF with the functions you have built earlier. \n",
    "2. Alternatively, the [`CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) function can be used to create your bag of words. Use the `max_features=5000` argument to only select the top 5000 most common words. \n",
    "3. You can also use the [`TfidfTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer) to help transform your bag of words with TFIDF.  \n",
    "  \n",
    "The dataframes for the train and test dataset has already been imported for you. You will have to make use of the skills you have learnt earlier and in Experience 1 to preprocess, vectorize (with the bag of words), transform (with TFIDF), fit, and predict the movie review sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer # This function helps you create your bag of words\n",
    "from sklearn.feature_extraction.text import TfidfTransformer # This function automatically normalizes your bag of words\n",
    "df_raw = pd.read_pickle('./imdb/df_raw.pkl')\n",
    "df_raw_test = pd.read_pickle('./imdb/df_raw_test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start off, try printing a sample of the test dataset. What are the names of the columns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>scores</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I went and saw this movie last night after bei...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Actor turned director Bill Paxton follows up h...</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>As a recreational golfer with some knowledge o...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I saw this film in a sneak preview, and it is ...</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bill Paxton has taken the true story of the 19...</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I saw this film on September 1st, 2005 in Indi...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Maybe I'm reading into this too much, but I wo...</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I felt this film did have many good qualities....</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>This movie is amazing because the fact that th...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\"Quitting\" may be as much about exiting a pre-...</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>I loved this movie from beginning to end.I am ...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>I was fortunate to attend the London premier o...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>I first saw this movie on IFC. Which is a grea...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>I must say, every time I see this movie, I am ...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>My wife is a mental health therapist and we wa...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>I saw this film at the Rotterdam International...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>\"Night of the Hunted\" stars French porn star B...</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Even if you're a fan of Jean Rollin's idiosync...</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>I was surprised how much I enjoyed this. Sure ...</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>I went into \"Night of the Hunted\" not knowing ...</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>I have certainly not seen all of Jean Rollin's...</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Since this cartoon was made in the old days, F...</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Despite the title and unlike some other storie...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Felix in Hollywood is a great film. The versio...</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>A gem of a cartoon from the silent era---it wa...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>This short is one of the best of all time and ...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Felix is watching an actor rehearse his lines:...</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>While I can't say whether or not Larry Hama ev...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Errol Flynn's roguish charm really shines thro...</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Warner Brothers tampered considerably with Ame...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12470</th>\n",
       "      <td>I was expecting \"Born to Kill\" to be an exciti...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12471</th>\n",
       "      <td>Fellow Giallo-fanatics: beware and/or proceed ...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12472</th>\n",
       "      <td>I'm almost embarrassed to admit to seeing CALI...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12473</th>\n",
       "      <td>I rated this movie a 3 and that was generous. ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12474</th>\n",
       "      <td>\"Caligula\" shares many of the same attributes ...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12475</th>\n",
       "      <td>Writer &amp; director Robert Downey, Sr., a pionee...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12476</th>\n",
       "      <td>Probably not the same version as most of the o...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12477</th>\n",
       "      <td>Who really wants to see that? Disgusting viole...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12478</th>\n",
       "      <td>As a semi-film buff, I had heard of this infam...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12479</th>\n",
       "      <td>210 minute version (extremely hardcore, or so ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12480</th>\n",
       "      <td>Although its plot is taken from the history of...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12481</th>\n",
       "      <td>I found the movie at my local video store and ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12482</th>\n",
       "      <td>As an ancient movie fan, I had heard much abou...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12483</th>\n",
       "      <td>Incomprehensibly dreadful mishmash of the prob...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12484</th>\n",
       "      <td>I'm currently slogging through Gibbon's 'Fall ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12485</th>\n",
       "      <td>It's a rare sensation to come across a film so...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12486</th>\n",
       "      <td>Starting with a tearjerking poem and images of...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12487</th>\n",
       "      <td>I saw this film at the tender age of 18 with a...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12488</th>\n",
       "      <td>It's rare to see film that strikes out in ever...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12489</th>\n",
       "      <td>Some describe CALIGULIA as \"the\" most controve...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12490</th>\n",
       "      <td>As I read the script on-line, I thought \"Capot...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12491</th>\n",
       "      <td>In this film, we're invited to observe the des...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12492</th>\n",
       "      <td>many people said this was a great movie with H...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12493</th>\n",
       "      <td>This is one dreary, inert, self-important bore...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12494</th>\n",
       "      <td>Awful, awful, awful times a hundred still does...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12495</th>\n",
       "      <td>I occasionally let my kids watch this garbage ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12496</th>\n",
       "      <td>When all we have anymore is pretty much realit...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12497</th>\n",
       "      <td>The basic genre is a thriller intercut with an...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12498</th>\n",
       "      <td>Four things intrigued me as to this film - fir...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12499</th>\n",
       "      <td>David Bryce's comments nearby are exceptionall...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  scores  positive\n",
       "0      I went and saw this movie last night after bei...      10         1\n",
       "1      Actor turned director Bill Paxton follows up h...       7         1\n",
       "2      As a recreational golfer with some knowledge o...       9         1\n",
       "3      I saw this film in a sneak preview, and it is ...       8         1\n",
       "4      Bill Paxton has taken the true story of the 19...       8         1\n",
       "5      I saw this film on September 1st, 2005 in Indi...       9         1\n",
       "6      Maybe I'm reading into this too much, but I wo...       8         1\n",
       "7      I felt this film did have many good qualities....       7         1\n",
       "8      This movie is amazing because the fact that th...      10         1\n",
       "9      \"Quitting\" may be as much about exiting a pre-...       8         1\n",
       "10     I loved this movie from beginning to end.I am ...      10         1\n",
       "11     I was fortunate to attend the London premier o...       9         1\n",
       "12     I first saw this movie on IFC. Which is a grea...       9         1\n",
       "13     I must say, every time I see this movie, I am ...       9         1\n",
       "14     My wife is a mental health therapist and we wa...       9         1\n",
       "15     I saw this film at the Rotterdam International...       9         1\n",
       "16     \"Night of the Hunted\" stars French porn star B...       7         1\n",
       "17     Even if you're a fan of Jean Rollin's idiosync...       8         1\n",
       "18     I was surprised how much I enjoyed this. Sure ...       8         1\n",
       "19     I went into \"Night of the Hunted\" not knowing ...       8         1\n",
       "20     I have certainly not seen all of Jean Rollin's...       8         1\n",
       "21     Since this cartoon was made in the old days, F...       8         1\n",
       "22     Despite the title and unlike some other storie...      10         1\n",
       "23     Felix in Hollywood is a great film. The versio...       8         1\n",
       "24     A gem of a cartoon from the silent era---it wa...       9         1\n",
       "25     This short is one of the best of all time and ...      10         1\n",
       "26     Felix is watching an actor rehearse his lines:...       8         1\n",
       "27     While I can't say whether or not Larry Hama ev...       9         1\n",
       "28     Errol Flynn's roguish charm really shines thro...       8         1\n",
       "29     Warner Brothers tampered considerably with Ame...      10         1\n",
       "...                                                  ...     ...       ...\n",
       "12470  I was expecting \"Born to Kill\" to be an exciti...       4         0\n",
       "12471  Fellow Giallo-fanatics: beware and/or proceed ...       4         0\n",
       "12472  I'm almost embarrassed to admit to seeing CALI...       1         0\n",
       "12473  I rated this movie a 3 and that was generous. ...       3         0\n",
       "12474  \"Caligula\" shares many of the same attributes ...       4         0\n",
       "12475  Writer & director Robert Downey, Sr., a pionee...       3         0\n",
       "12476  Probably not the same version as most of the o...       2         0\n",
       "12477  Who really wants to see that? Disgusting viole...       1         0\n",
       "12478  As a semi-film buff, I had heard of this infam...       1         0\n",
       "12479  210 minute version (extremely hardcore, or so ...       3         0\n",
       "12480  Although its plot is taken from the history of...       4         0\n",
       "12481  I found the movie at my local video store and ...       1         0\n",
       "12482  As an ancient movie fan, I had heard much abou...       2         0\n",
       "12483  Incomprehensibly dreadful mishmash of the prob...       1         0\n",
       "12484  I'm currently slogging through Gibbon's 'Fall ...       1         0\n",
       "12485  It's a rare sensation to come across a film so...       1         0\n",
       "12486  Starting with a tearjerking poem and images of...       2         0\n",
       "12487  I saw this film at the tender age of 18 with a...       1         0\n",
       "12488  It's rare to see film that strikes out in ever...       1         0\n",
       "12489  Some describe CALIGULIA as \"the\" most controve...       4         0\n",
       "12490  As I read the script on-line, I thought \"Capot...       2         0\n",
       "12491  In this film, we're invited to observe the des...       3         0\n",
       "12492  many people said this was a great movie with H...       2         0\n",
       "12493  This is one dreary, inert, self-important bore...       2         0\n",
       "12494  Awful, awful, awful times a hundred still does...       2         0\n",
       "12495  I occasionally let my kids watch this garbage ...       1         0\n",
       "12496  When all we have anymore is pretty much realit...       1         0\n",
       "12497  The basic genre is a thriller intercut with an...       3         0\n",
       "12498  Four things intrigued me as to this film - fir...       3         0\n",
       "12499  David Bryce's comments nearby are exceptionall...       4         0\n",
       "\n",
       "[25000 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, process your text using the `CountVectorizer` to create your bag of words. you can create a class of `CountVectorizer()`, and use the method `.fit_transform()` with the text as argument to build your bag of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer = \"word\", strip_accents=None, tokenizer = None, \\\n",
    "                             preprocessor = None, stop_words = None, max_features = 5000) \n",
    "train_data_features = vectorizer.fit_transform(df_raw['text'])\n",
    "test_data_features = vectorizer.transform(df_raw_test['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now normalize your bag of words using the `TfidfTransformer`. Use it the same way as above. Create a class, and use the `.fit_transform()` method with the bag of words as your argument to create your TFIDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfier = TfidfTransformer()\n",
    "tfidf = tfidfier.fit_transform(train_data_features)\n",
    "tfidf_test = tfidfier.transform(test_data_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use your transformed bag of words as the features to train and test your model like we did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\data_idp3e\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.88256\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_all = tfidf.toarray()\n",
    "y_all = df_raw['positive'].values\n",
    "X_test = tfidf_test.toarray()\n",
    "y_test = df_raw_test['positive'].values\n",
    "def classify():\n",
    "    rf = LogisticRegression()\n",
    "    rf.fit(X_all,y_all)\n",
    "    print(rf.score(X_test,y_test))\n",
    "    return rf\n",
    "classify()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should have an accuracy of 80% without any hyperparameter tuning. Try and get the best accuracy on the test set that you can! This dataset is one of the hallmarks of natural language processing and is the entry-point for many aspiring data scientists and engineers. You can [see the original competition here](https://www.kaggle.com/c/word2vec-nlp-tutorial), and look at the different solutions other people have created!  \n",
    "\n",
    "## Next up, you can create a function to input your own review and get the model to predict if your sentence is positive or negative!\n",
    "\n",
    "# Congratulations!\n",
    "You have learnt how to build a natural language text classifier! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
