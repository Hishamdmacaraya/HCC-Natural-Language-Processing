{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging and Problem Solving in Text Cleaning and Preprocessing\n",
    "\n",
    "Coder: Hisham D Macaraya\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "This project focuses on the process of text cleaning and preprocessing using Python. It provides an opportunity to practice debugging Python code that is intended to clean and prepare a set of user reviews for further natural language processing (NLP) tasks. Preprocessing text data is an essential part of NLP, as it transforms raw text into a format that can be effectively used by machine learning models.\n",
    "\n",
    "In this exercise, intentional errors have been introduced into the given code, and your goal is to identify and correct these errors. By doing so, you will improve your understanding of text preprocessing techniques such as tokenization, lowercasing, punctuation removal, stopwords removal, and stemming.\n",
    "\n",
    "### Objectives:\n",
    "1. Identify and fix the errors.\n",
    "2. Ensure that the code tokenizes the reviews, converts them to lowercase, removes punctuation and stopwords, and then applies stemming.\n",
    "3. Compare your results to what you expect them to be based on your understanding of the process.\n",
    "\n",
    "**Remember:** The process of debugging is not just about making the code run without errorsâ€”it's also about ensuring the code produces the correct and expected outcomes. Use print statements or any other method you prefer to check intermediate results.\n",
    "\n",
    "#### Sample Dataset: User Reviews\n",
    "```python\n",
    "reviews = [\n",
    "    \"Best purchase I made this year!!\",\n",
    "    \"Totally regret this. Stopped after a month.\",\n",
    "    \"Average product. Could be better, could be worse.\",\n",
    "    \"Impressed with the quality and performance.\",\n",
    "    \"Never buying this again. Complete waste.\"\n",
    "]\n",
    "```\n",
    "\n",
    "#### Task 1: Tokenization (ERROR INTRODUCED HERE)\n",
    "```python\n",
    "tokenized_reviews = [word_tokenize(reviews) for review in reviews]\n",
    "```\n",
    "\n",
    "#### Task 2: Lowercasing (ERROR INTRODUCED HERE)\n",
    "```python\n",
    "lowercased_reviews = [[word.lower for word in review] for review in tokenized_reviews]\n",
    "```\n",
    "\n",
    "#### Task 3: Removing Punctuation and Stopwords\n",
    "```python\n",
    "stop_words = set(stopwords.words('english'))\n",
    "cleaned_reviews = [[word for word in review if word.isalnum() and word not in stop_words] for review in lowercased_reviews]\n",
    "```\n",
    "\n",
    "#### Task 4: Stemming (ERROR INTRODUCED HERE)\n",
    "```python\n",
    "stemmed_reviews = [[stemmer.stemming(word) for word in review] for review in cleaned_reviews]\n",
    "```\n",
    "\n",
    "#### Print Out the Stemmed Reviews\n",
    "```python\n",
    "for r in stemmed_reviews:\n",
    "    print(' '.join(r))\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Necessary Libraries and Packages\n",
    "\n",
    "### Importing Libraries\n",
    "In this section, we will import the necessary libraries for natural language processing (NLP), including tokenization, stopwords, and stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing NLTK library and specific modules for natural language processing tasks\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize  # For tokenizing sentences into words\n",
    "from nltk.corpus import stopwords  # For accessing a list of common stop words\n",
    "from nltk.stem import PorterStemmer  # For stemming words to their root form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download required NLTK data\n",
    "\n",
    "### Downloading NLTK Data\n",
    "We need to download 'punkt' for tokenization and 'stopwords' for removing common words that do not add much value to the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hisha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hisha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'punkt' is used for tokenization, and 'stopwords' provides a list of common words to be removed\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample dataset: User reviews\n",
    "\n",
    "### Sample Dataset: User Reviews\n",
    "Here we define a list of user reviews for a hypothetical product. These reviews will be processed through different NLP techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a list of user reviews for a hypothetical product\n",
    "reviews = [\n",
    "    \"Best purchase I made this year!!\",\n",
    "    \"Totally regret this. Stopped after a month.\",\n",
    "    \"Average product. Could be better, could be worse.\",\n",
    "    \"Impressed with the quality and performance.\",\n",
    "    \"Never buying this again. Complete waste.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Tokenization (FIXED)\n",
    "\n",
    "### Task 1: Tokenization\n",
    "Tokenizing each review into individual words. Tokenization helps break down text into smaller units (words) for further analysis.\n",
    "\n",
    "#### Original Error\n",
    "The original code had 'word_tokenize(reviews)' instead of 'word_tokenize(review)'. This resulted in an error because 'reviews' is a list, and we need to tokenize each individual review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens for each review:\n",
      "Review 1 tokens: ['Best', 'purchase', 'I', 'made', 'this', 'year', '!', '!']\n",
      "Review 2 tokens: ['Totally', 'regret', 'this', '.', 'Stopped', 'after', 'a', 'month', '.']\n",
      "Review 3 tokens: ['Average', 'product', '.', 'Could', 'be', 'better', ',', 'could', 'be', 'worse', '.']\n",
      "Review 4 tokens: ['Impressed', 'with', 'the', 'quality', 'and', 'performance', '.']\n",
      "Review 5 tokens: ['Never', 'buying', 'this', 'again', '.', 'Complete', 'waste', '.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing each review into individual words\n",
    "tokenized_reviews = [word_tokenize(review) for review in reviews]\n",
    "print(\"Tokens for each review:\")\n",
    "for i, tokens in enumerate(tokenized_reviews):\n",
    "    print(f\"Review {i + 1} tokens: {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Lowercasing (FIXED)\n",
    "\n",
    "### Task 2: Lowercasing\n",
    "Converting all tokens to lowercase for consistency. This ensures that words like 'Product' and 'product' are treated as the same word.\n",
    "\n",
    "#### Original Error\n",
    "The original code used 'word.lower' instead of 'word.lower()'. This resulted in an error since 'lower' is a method and must be called with parentheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lowercased tokens for each review:\n",
      "Review 1 lowercased: ['best', 'purchase', 'i', 'made', 'this', 'year', '!', '!']\n",
      "Review 2 lowercased: ['totally', 'regret', 'this', '.', 'stopped', 'after', 'a', 'month', '.']\n",
      "Review 3 lowercased: ['average', 'product', '.', 'could', 'be', 'better', ',', 'could', 'be', 'worse', '.']\n",
      "Review 4 lowercased: ['impressed', 'with', 'the', 'quality', 'and', 'performance', '.']\n",
      "Review 5 lowercased: ['never', 'buying', 'this', 'again', '.', 'complete', 'waste', '.']\n"
     ]
    }
   ],
   "source": [
    "# Converting all tokens to lowercase for consistency\n",
    "lowercased_reviews = [[word.lower() for word in review] for review in tokenized_reviews]\n",
    "print(\"\\nLowercased tokens for each review:\")\n",
    "for i, tokens in enumerate(lowercased_reviews):\n",
    "    print(f\"Review {i + 1} lowercased: {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Removing Punctuation and Stopwords\n",
    "\n",
    "### Task 3: Removing Punctuation and Stopwords\n",
    "Removing punctuation and common stop words from the reviews. Stop words are common words that do not contribute much meaning, such as 'the', 'is', etc.\n",
    "\n",
    "#### Method\n",
    "We use 'isalnum()' to check if the word is alphanumeric, which helps remove punctuation. Additionally, stop words are filtered out using the NLTK stopwords list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaned tokens for each review (no punctuation and stopwords):\n",
      "Review 1 cleaned: ['best', 'purchase', 'made', 'year']\n",
      "Review 2 cleaned: ['totally', 'regret', 'stopped', 'month']\n",
      "Review 3 cleaned: ['average', 'product', 'could', 'better', 'could', 'worse']\n",
      "Review 4 cleaned: ['impressed', 'quality', 'performance']\n",
      "Review 5 cleaned: ['never', 'buying', 'complete', 'waste']\n"
     ]
    }
   ],
   "source": [
    "# Removing punctuation and common stop words from the reviews\n",
    "stop_words = set(stopwords.words('english'))\n",
    "cleaned_reviews = [[word for word in review if word.isalnum() and word not in stop_words] for review in lowercased_reviews]\n",
    "print(\"\\nCleaned tokens for each review (no punctuation and stopwords):\")\n",
    "for i, tokens in enumerate(cleaned_reviews):\n",
    "    print(f\"Review {i + 1} cleaned: {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Stemming (FIXED)\n",
    "\n",
    "### Task 4: Stemming\n",
    "Applying stemming to reduce words to their root forms. Stemming helps normalize words to a common base form, for example, 'running' to 'run'.\n",
    "\n",
    "#### Original Error\n",
    "The original code used 'stemmer.stemming(word)' instead of 'stemmer.stem(word)'. The correct method is 'stem', which returns the stemmed version of the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stemmed tokens for each review:\n",
      "Review 1 stemmed: ['best', 'purchas', 'made', 'year']\n",
      "Review 2 stemmed: ['total', 'regret', 'stop', 'month']\n",
      "Review 3 stemmed: ['averag', 'product', 'could', 'better', 'could', 'wors']\n",
      "Review 4 stemmed: ['impress', 'qualiti', 'perform']\n",
      "Review 5 stemmed: ['never', 'buy', 'complet', 'wast']\n"
     ]
    }
   ],
   "source": [
    "# Applying stemming to reduce words to their root forms\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_reviews = [[stemmer.stem(word) for word in review] for review in cleaned_reviews]\n",
    "print(\"\\nStemmed tokens for each review:\")\n",
    "for i, tokens in enumerate(stemmed_reviews):\n",
    "    print(f\"Review {i + 1} stemmed: {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Output\n",
    "\n",
    "### Final Output: Processed Reviews\n",
    "Printing the stemmed reviews to show the final processed form of each review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Processed Reviews:\n",
      "Review 1: best purchas made year\n",
      "Review 2: total regret stop month\n",
      "Review 3: averag product could better could wors\n",
      "Review 4: impress qualiti perform\n",
      "Review 5: never buy complet wast\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nFinal Processed Reviews:\")\n",
    "for i, r in enumerate(stemmed_reviews):\n",
    "    print(f\"Review {i + 1}: {' '.join(r)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
