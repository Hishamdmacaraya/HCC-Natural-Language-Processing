{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scikit-learn: Working With Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exersice 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "# Custom preprocessor function to clean the text before vectorization\n",
    "# Here, we convert the text to lowercase for normalization\n",
    "# Additional preprocessing steps can be added as needed\n",
    "def custom_preprocessor(text):\n",
    "    # Convert text to lowercase to ensure case-insensitivity\n",
    "    text = text.lower()\n",
    "    # Remove non-alphabetic characters if needed (optional)\n",
    "    return text\n",
    "\n",
    "# Function to load the dataset from the specified directory\n",
    "# Assumes that there are subdirectories for each language, with text files inside each subdirectory\n",
    "# Each subdirectory's name is treated as the label (i.e., language)\n",
    "def load_data(directory):\n",
    "    texts, labels = [], []  # Initialize empty lists to store texts and labels\n",
    "    for label in os.listdir(directory):  # Loop through each subdirectory (each representing a language)\n",
    "        label_dir = os.path.join(directory, label)  # Path to the subdirectory\n",
    "        if os.path.isdir(label_dir):  # Check if the path is indeed a directory\n",
    "            for filename in os.listdir(label_dir):  # Loop through each file in the subdirectory\n",
    "                filepath = os.path.join(label_dir, filename)  # Full path to the file\n",
    "                with open(filepath, 'r', encoding='utf-8') as f:  # Open the file\n",
    "                    texts.append(f.read())  # Append the file content (text) to the texts list\n",
    "                    labels.append(label)  # Append the label (language) to the labels list\n",
    "    return texts, labels  # Return the collected texts and labels\n",
    "\n",
    "# Specify the directory where the dataset is located\n",
    "# Adjust the path as per your folder structure\n",
    "data_dir = 'data/languages/paragraphs/'  # Path to the directory containing language data\n",
    "\n",
    "# Load the dataset into two lists: texts (the text data) and labels (the corresponding language labels)\n",
    "texts, labels = load_data(data_dir)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "# We use 80% of the data for training and 20% for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the text classification pipeline\n",
    "# This pipeline consists of two main components:\n",
    "# 1. TfidfVectorizer: Converts the text into numerical features using character-based n-grams\n",
    "# 2. SGDClassifier: A linear Support Vector Machine (SVM) classifier that is suitable for text classification\n",
    "text_clf = Pipeline([\n",
    "    # TfidfVectorizer:\n",
    "    # - analyzer='char': This means we are using character-level n-grams\n",
    "    # - ngram_range=(3, 5): We use n-grams of lengths 3 to 5 (this captures character sequences of different lengths)\n",
    "    # - preprocessor=custom_preprocessor: Use the custom preprocessor function to clean the text\n",
    "    ('tfidf', TfidfVectorizer(analyzer='char', ngram_range=(3, 5), preprocessor=custom_preprocessor)),\n",
    "    \n",
    "    # SGDClassifier:\n",
    "    # - loss='hinge': This specifies the hinge loss function, commonly used for SVM\n",
    "    # - penalty='l2': L2 regularization to prevent overfitting\n",
    "    # - alpha=1e-3: Regularization strength (smaller values mean stronger regularization)\n",
    "    # - max_iter=5, tol=None: Maximum number of iterations for training, and no tolerance for stopping early\n",
    "    ('clf', SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, random_state=42, max_iter=5, tol=None)),\n",
    "])\n",
    "\n",
    "# Train the model by fitting the pipeline on the training data (X_train, y_train)\n",
    "text_clf.fit(X_train, y_train)\n",
    "\n",
    "# Use the trained model to predict the labels of the test data (X_test)\n",
    "y_pred = text_clf.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the model using classification metrics\n",
    "# - classification_report: This generates precision, recall, and F1-score for each class (language)\n",
    "# - confusion_matrix: This generates a confusion matrix to show misclassifications\n",
    "print(metrics.classification_report(y_test, y_pred))  # Detailed performance metrics\n",
    "print(metrics.confusion_matrix(y_test, y_pred))  # Confusion matrix showing actual vs predicted labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn import metrics\n",
    "\n",
    "# Function to load movie review data from the given directory\n",
    "# Assumes there are two subdirectories: 'pos' for positive reviews and 'neg' for negative reviews\n",
    "# Each subdirectory contains multiple text files, each representing a single review\n",
    "def load_data(directory):\n",
    "    texts, labels = [], []  # Initialize empty lists to store texts and their corresponding labels\n",
    "    for label in ['pos', 'neg']:  # We assume the dataset has 'pos' and 'neg' subdirectories\n",
    "        label_dir = os.path.join(directory, label)  # Get the full path to each subdirectory\n",
    "        for filename in os.listdir(label_dir):  # Loop through each file in the subdirectory\n",
    "            filepath = os.path.join(label_dir, filename)  # Full path to the text file\n",
    "            with open(filepath, 'r', encoding='utf-8') as f:  # Open the file with UTF-8 encoding\n",
    "                texts.append(f.read())  # Read the content of the file and append to texts list\n",
    "                labels.append(label)  # Append 'pos' or 'neg' as the label for this review\n",
    "    return texts, labels  # Return the list of texts and their labels\n",
    "\n",
    "# Specify the directory where the movie review dataset is located\n",
    "# The path should point to the folder containing 'pos' and 'neg' subdirectories\n",
    "data_dir = 'data/movie_reviews/txt_sentoken/'  # Adjust this path based on your folder structure\n",
    "\n",
    "# Load the movie review dataset into two lists: \n",
    "# - texts: contains the review text from each file\n",
    "# - labels: contains the corresponding label ('pos' or 'neg') for each review\n",
    "texts, labels = load_data(data_dir)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "# - X_train: training data (80% of the reviews)\n",
    "# - X_test: testing data (20% of the reviews)\n",
    "# - y_train: training labels (80% corresponding to X_train)\n",
    "# - y_test: testing labels (20% corresponding to X_test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build a text classification pipeline\n",
    "# - TfidfVectorizer: Converts text data into numerical features using Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "# - SGDClassifier: A linear SVM classifier trained using Stochastic Gradient Descent (SGD)\n",
    "text_clf = Pipeline([\n",
    "    # TfidfVectorizer:\n",
    "    # - stop_words='english': Removes common English stop words (e.g., 'the', 'is', 'and')\n",
    "    # - This transforms the review texts into numerical vectors\n",
    "    ('tfidf', TfidfVectorizer(stop_words='english')),\n",
    "    \n",
    "    # SGDClassifier:\n",
    "    # - A linear SVM classifier that uses the hinge loss function\n",
    "    # - random_state: Seed to ensure reproducibility of results\n",
    "    # - max_iter=1000: Allows the classifier to iterate up to 1000 times over the data\n",
    "    # - tol=1e-3: The tolerance for stopping criteria (stops training once the model converges)\n",
    "    ('clf', SGDClassifier(random_state=42, max_iter=1000, tol=1e-3)),\n",
    "])\n",
    "\n",
    "# Perform grid search to find the best set of parameters\n",
    "# Parameters:\n",
    "# - 'tfidf__ngram_range': Check both unigrams (1,1) and bigrams (1,2)\n",
    "# - 'tfidf__use_idf': Whether to use IDF (inverse document frequency) scaling\n",
    "# - 'clf__alpha': The regularization strength for the SVM classifier (smaller values mean stronger regularization)\n",
    "parameters = {\n",
    "    'tfidf__ngram_range': [(1, 1), (1, 2)],  # Unigrams or bigrams\n",
    "    'tfidf__use_idf': [True, False],  # Use or not use inverse document frequency\n",
    "    'clf__alpha': [1e-3, 1e-4],  # Regularization strength for SVM\n",
    "}\n",
    "\n",
    "# Use GridSearchCV to perform an exhaustive search over parameter combinations\n",
    "# - cv=5: Perform 5-fold cross-validation for more reliable estimates\n",
    "# - n_jobs=-1: Use all available cores to parallelize the grid search\n",
    "gs_clf = GridSearchCV(text_clf, parameters, cv=5, n_jobs=-1)\n",
    "gs_clf.fit(X_train, y_train)  # Train the model with the best parameters found\n",
    "\n",
    "# Print the best parameters found by the grid search\n",
    "# This helps identify which parameter combination performed best during cross-validation\n",
    "print(\"Best parameters found:\")\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))\n",
    "\n",
    "# Evaluate the performance of the model on the test set\n",
    "# The model is tested on data it has never seen before (X_test) to evaluate how well it generalizes\n",
    "y_pred = gs_clf.predict(X_test)  # Make predictions on the test data\n",
    "\n",
    "# Print the classification report:\n",
    "# - precision: The number of correct positive predictions divided by the total number of positive predictions\n",
    "# - recall: The number of correct positive predictions divided by the actual number of positives in the data\n",
    "# - f1-score: The harmonic mean of precision and recall (provides a balance between the two)\n",
    "# - support: The number of actual occurrences of each class ('pos' and 'neg')\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "\n",
    "# Print the confusion matrix:\n",
    "# - Shows how often the model correctly predicted the class (on-diagonal elements) and how often it misclassified (off-diagonal elements)\n",
    "# - The rows represent the actual class, and the columns represent the predicted class\n",
    "print(metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for training and pickling the models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Assume that `text_clf_language` is the language detection pipeline from Exercise 1\n",
    "# and `text_clf_sentiment` is the sentiment analysis pipeline from Exercise 2\n",
    "\n",
    "# Train and pickle the language detection model\n",
    "def save_language_model():\n",
    "    # Train the language detection model (from Exercise 1)\n",
    "    text_clf_language.fit(X_train_language, y_train_language)  # Use training data for language detection\n",
    "    \n",
    "    # Save the trained model to a file\n",
    "    with open('language_model.pkl', 'wb') as f:\n",
    "        pickle.dump(text_clf_language, f)\n",
    "\n",
    "# Train and pickle the sentiment analysis model\n",
    "def save_sentiment_model():\n",
    "    # Train the sentiment analysis model (from Exercise 2)\n",
    "    text_clf_sentiment.fit(X_train_sentiment, y_train_sentiment)  # Use training data for sentiment analysis\n",
    "    \n",
    "    # Save the trained model to a file\n",
    "    with open('sentiment_model.pkl', 'wb') as f:\n",
    "        pickle.dump(text_clf_sentiment, f)\n",
    "\n",
    "# Call these functions after training to save the models\n",
    "save_language_model()\n",
    "save_sentiment_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for the CLI utility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle\n",
    "import argparse\n",
    "from sklearn import metrics\n",
    "\n",
    "# Load pickled models\n",
    "with open('language_model.pkl', 'rb') as f:\n",
    "    language_model = pickle.load(f)\n",
    "\n",
    "with open('sentiment_model.pkl', 'rb') as f:\n",
    "    sentiment_model = pickle.load(f)\n",
    "\n",
    "# Command-line utility for text classification\n",
    "def classify_text(input_text):\n",
    "    # Step 1: Language Detection\n",
    "    language_pred = language_model.predict([input_text])\n",
    "    language_confidence = max(language_model.predict_proba([input_text])[0])  # Get max confidence score\n",
    "\n",
    "    print(f\"Detected language: {language_pred[0]} with confidence: {language_confidence:.4f}\")\n",
    "\n",
    "    # Step 2: Sentiment Analysis (if English is detected)\n",
    "    if language_pred[0] == 'english':  # Assuming 'english' is the label for English language\n",
    "        sentiment_pred = sentiment_model.predict([input_text])\n",
    "        sentiment_confidence = max(sentiment_model.predict_proba([input_text])[0])  # Get max confidence score\n",
    "        \n",
    "        sentiment_label = 'positive' if sentiment_pred[0] == 'pos' else 'negative'\n",
    "        print(f\"Sentiment: {sentiment_label} with confidence: {sentiment_confidence:.4f}\")\n",
    "    else:\n",
    "        print(\"Sentiment analysis is only available for English text.\")\n",
    "\n",
    "# Set up argument parsing\n",
    "parser = argparse.ArgumentParser(description=\"CLI utility for language detection and sentiment analysis.\")\n",
    "parser.add_argument('--text', type=str, help=\"Text to classify. If not provided, input will be read from stdin.\")\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "# If --text argument is provided, classify that text, otherwise read from stdin\n",
    "if args.text:\n",
    "    classify_text(args.text)\n",
    "else:\n",
    "    print(\"Please provide some input text:\")\n",
    "    input_text = sys.stdin.read().strip()\n",
    "    classify_text(input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breakdown of the CLI Utility:\n",
    "\n",
    "### Model loading:\n",
    "- The models are loaded from their respective pickled files (`language_model.pkl` and `sentiment_model.pkl`).\n",
    "\n",
    "### Classification process:\n",
    "- The utility first predicts the **language** of the input text using the language detection model.\n",
    "- If the detected language is **English**, the sentiment analysis model is used to predict whether the sentiment is **positive** or **negative**.\n",
    "\n",
    "### Confidence score:\n",
    "- Both the language detection and sentiment analysis models provide a confidence score by using the `predict_proba` method.\n",
    "- The highest score from the predicted probabilities is displayed as the confidence level.\n",
    "\n",
    "### Command-line arguments:\n",
    "- The utility can accept input through the `--text` argument.\n",
    "- If no text is provided via the argument, the utility will prompt for input via **stdin**.\n",
    "\n",
    "### Usage:\n",
    "\n",
    "#### To classify a string provided as a command-line argument:\n",
    "```bash\n",
    "cli_text_classification.py --text \"This is a great movie!\"\n",
    "\n",
    "```\n",
    "\n",
    "#### To classify text input from stdin:\n",
    "```bash\n",
    "echo \"I love this movie!\" | python cli_text_classification.py\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
