{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: Word Embedding with Keras\n",
    "\n",
    "Coder: Hisham D Macaraya\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Deep Learning in NLP\n",
    "\n",
    "For an introduction to deep learning in NLP, we'll start with a simple program that demonstrates the use of word embeddings using the TensorFlow and Keras libraries. Specifically, we'll implement a word embedding layer and visualize the embeddings for a few words using the popular Embedding layer in Keras. This will provide a hands-on introduction to word embeddings in deep learning.\n",
    "\n",
    "### Instructions:\n",
    "\n",
    "1. Ensure you have TensorFlow installed in your environment. You can install it via pip: `pip install tensorflow`\n",
    "\n",
    "2. Run the code above. This code will train a simple neural network model with an embedding layer on some sample sentences.\n",
    "\n",
    "3. Observe the embedding for the word 'learning'. This is an 8-dimensional vector representing the word 'learning' in the learned embedding space.\n",
    "\n",
    "4. Experiment by adding your sentences and observing how the embeddings change.\n",
    "\n",
    "5. Optionally, you can use tools like TensorBoard to visualize the high-dimensional embeddings in 2D or 3D space.\n",
    "\n",
    "This exercise will help you understand the fundamental concept of word embeddings and how they can be obtained using deep learning libraries like Keras. The primary goal is to familiarize you with the idea, and more advanced concepts will follow in subsequent modules.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hisha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.5000 - loss: 0.6872\n",
      "Epoch 2/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.5000 - loss: 0.6824\n",
      "Epoch 3/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.5000 - loss: 0.6781\n",
      "Epoch 4/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.7500 - loss: 0.6740\n",
      "Epoch 5/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 0.6704\n",
      "Epoch 6/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 0.6669\n",
      "Epoch 7/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.6634\n",
      "Epoch 8/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.6599\n",
      "Epoch 9/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.6563\n",
      "Epoch 10/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.6527\n",
      "Embedding for the word 'learning': [ 0.01873598  0.00679409  0.01551568  0.03084649  0.05914946 -0.04510757\n",
      "  0.00228451 -0.01626277]\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Dense\n",
    "\n",
    "# Step 1: Define Sample Sentences\n",
    "# These sentences will be used to train the embedding layer\n",
    "sentences = [\n",
    "    \"I love machine learning\",\n",
    "    \"Deep learning is a subfield of machine learning\",\n",
    "    \"NLP stands for natural language processing\",\n",
    "    \"Embeddings are useful in NLP\"\n",
    "]\n",
    "\n",
    "# Step 2: Tokenize the Sentences\n",
    "# Tokenizer will convert the words to integer indices\n",
    "# The oov_token parameter is used to handle out-of-vocabulary words\n",
    "# Limit to 50 words\n",
    "\n",
    "# Tokenize the sentences\n",
    "tokenizer = Tokenizer(num_words=50, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Convert the sentences to sequences and pad them to ensure consistent length\n",
    "# Padding is applied to make all sequences the same length\n",
    "# Padding applied at the end (post padding)\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "padded_sequences = pad_sequences(sequences, padding='post')\n",
    "\n",
    "# Step 3: Create a Neural Network Model with an Embedding Layer\n",
    "# The embedding layer will learn an 8-dimensional representation of the words\n",
    "model = Sequential([\n",
    "    Embedding(50, 8, input_length=padded_sequences.shape[1]),  # Embedding layer with 50 words and 8-dimensional vectors\n",
    "    Flatten(),  # Flatten the embeddings\n",
    "    Dense(6, activation='relu'),  # Intermediate dense layer for demonstration\n",
    "    Dense(1, activation='sigmoid')  # Dummy output layer\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "# Use Adam optimizer and binary cross-entropy loss\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Step 4: Fit the Model\n",
    "# Use dummy labels (just for demonstration purposes)\n",
    "labels = np.array([1, 0, 0, 1])  # Dummy labels corresponding to the sentences\n",
    "model.fit(padded_sequences, labels, epochs=10)\n",
    "\n",
    "# Step 5: Extract Embeddings\n",
    "# Extract and display the learned embeddings for a given word\n",
    "embedding_layer = model.layers[0]\n",
    "embedding_weights = embedding_layer.get_weights()[0]\n",
    "\n",
    "# Find the embedding for the word \"learning\"\n",
    "word_index_for_learning = word_index['learning']\n",
    "print(f\"Embedding for the word 'learning': {embedding_weights[word_index_for_learning]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "\n",
    "In this activity, I implemented a simple deep learning model using TensorFlow and Keras to explore word embeddings in natural language processing (NLP). I started by defining a set of sample sentences that would serve as the basis for training the model. After tokenizing these sentences into integer indices, I prepared the data for the model.\n",
    "\n",
    "The model featured an embedding layer designed to represent words as 8-dimensional vectors, which is crucial for capturing the semantic relationships between words. As I progressed through the training process, I noticed that the model initially had a 50% accuracy, indicating that it was struggling to learn effectively. However, as training continued over ten epochs, the model's performance improved dramatically, eventually achieving an accuracy of 100% by the fifth epoch. The loss value decreased steadily, further confirming that the model was learning from the data.\n",
    "\n",
    "After training, I extracted the embedding for the word \"learning,\" which resulted in an 8-dimensional vector that represented this word in the learned embedding space. This vector revealed how the model captures the semantic meaning of words, showcasing the effectiveness of word embeddings in NLP tasks. Overall, this exercise provided me with valuable insights into the fundamental concept of word embeddings and how deep learning libraries like Keras can be used to generate them, setting the stage for more advanced applications in future modules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
